{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":772,"status":"ok","timestamp":1694189254048,"user":{"displayName":"TEB231_Danish Maniyar","userId":"02254112466939208833"},"user_tz":-330},"id":"SR9SI9R_0WOU","outputId":"f0bf5135-f889-4c7e-a919-76072c954f61"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fri Sep  8 16:07:33 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","source":["#!pip install django\n"],"metadata":{"id":"m7elPYVtQkLU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!django-admin startproject dansih"],"metadata":{"id":"2UKOUdLSQ4eR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!ls"],"metadata":{"id":"MScLiH0iRBSU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#from google.colab.output import eval_js\n","print(eval_js(\"google.colab.kernel.proxyPort(8000)\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":199},"id":"X5VoUxNKRFPw","executionInfo":{"status":"error","timestamp":1694189254552,"user_tz":-330,"elapsed":22,"user":{"displayName":"TEB231_Danish Maniyar","userId":"02254112466939208833"}},"outputId":"76a0dccf-3b36-4f4f-c753-414f0005b75f"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-d51b0edea0a8>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#from google.colab.output import eval_js\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"google.colab.kernel.proxyPort(8000)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'eval_js' is not defined"]}]},{"cell_type":"code","source":[],"metadata":{"id":"WA-aPAP6Rm5U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!python manage.py runserver 800"],"metadata":{"id":"t9j-at12RW9V"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"je2tvd8Gu7YZ"},"outputs":[],"source":["import torch\n","!nvcc --version\n","TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n","CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n","print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6qsQaKLF0et7"},"outputs":[],"source":["import os\n","HOME = os.getcwd()\n","print(HOME)"]},{"cell_type":"markdown","metadata":{"id":"m2QOd7O0Xw3G"},"source":["## Installing YOLOv5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xV9ROeyqXwPN"},"outputs":[],"source":["%cd {HOME}\n","!git clone https://github.com/ultralytics/yolov5\n","\n","%cd {HOME}/yolov5\n","!pip install -r requirements.txt\n","\n","from IPython import display\n","display.clear_output()"]},{"cell_type":"markdown","metadata":{"id":"Ern2bvVG04GW"},"source":["## Installing YOLOv8"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8w4nepQU00rY"},"outputs":[],"source":["!pip install ultralytics\n","\n","from IPython import display\n","display.clear_output()\n","\n","import ultralytics\n","ultralytics.checks()"]},{"cell_type":"markdown","metadata":{"id":"OIx3WRFMcdLz"},"source":["## Installing Detectron2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QHsKVVoAcf3k"},"outputs":[],"source":["!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n","\n","from IPython import display\n","display.clear_output()\n","\n","import detectron2\n","print(\"detectron2:\", detectron2.__version__)"]},{"cell_type":"markdown","metadata":{"id":"2BY9KJHd1WER"},"source":["## Installing Supervision"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0bx_iBer09rD"},"outputs":[],"source":["!pip install supervision==0.2.0\n","\n","from IPython import display\n","display.clear_output()\n","\n","import supervision as sv\n","print(\"supervision\", sv.__version__)"]},{"cell_type":"markdown","metadata":{"id":"k2gAtbBwx6xt"},"source":["## Downloading the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aMQlu-7Dx9an"},"outputs":[],"source":["#%cd {HOME}\n","#!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1vVrEVMxucHgqGd7vAa501ASojbeGPhIr' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1vVrEVMxucHgqGd7vAa501ASojbeGPhIr\" -O market-square.mp4 && rm -rf /tmp/cookies.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-4fpqQB34uTu"},"outputs":[],"source":["MARKET_SQUARE_VIDEO_PATH = f\"{HOME}/mall.mp4\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o7ggrfW26qzp"},"outputs":[],"source":["%cd {HOME}\n","!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1M3UuH3QNDWGiH0NmGgHtIgXXGDo_nigm' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1M3UuH3QNDWGiH0NmGgHtIgXXGDo_nigm\" -O mall.mp4 && rm -rf /tmp/cookies.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-dPVr5oA7Lca"},"outputs":[],"source":["MALL_VIDEO_PATH = f\"{HOME}/mall.mp4\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"roNx4YTV-Ypd"},"outputs":[],"source":["%cd {HOME}\n","!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1qZ6ROKdzHbQiHdizKfYbecr9qquOQ0Cz' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1qZ6ROKdzHbQiHdizKfYbecr9qquOQ0Cz\" -O subway.mp4 && rm -rf /tmp/cookies.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CognP5tQ-j7i"},"outputs":[],"source":["SUBWAY_VIDEO_PATH = f\"{HOME}/subway.mp4\""]},{"cell_type":"markdown","metadata":{"id":"e59Bru1f4c_U"},"source":["## YOLOv8 Shopping Mall Example"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J5etlvT_ZTmB"},"outputs":[],"source":["from ultralytics import YOLO\n","\n","model = YOLO('yolov8s.pt')"]},{"cell_type":"markdown","metadata":{"id":"pe3pKKa7eGYm"},"source":["Let's start vanilla YOLOv8 inference pipeline."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XOB4Y0uS7boa"},"outputs":[],"source":["import supervision as sv\n","\n","# extract video frame\n","generator = sv.get_video_frames_generator(MALL_VIDEO_PATH)\n","iterator = iter(generator)\n","frame = next(iterator)\n","\n","# detect\n","results = model(frame, imgsz=1280)[0]\n","detections = sv.Detections.from_yolov8(results)\n","\n","# annotate\n","box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n","frame = box_annotator.annotate(scene=frame, detections=detections)\n","\n","%matplotlib inline\n","sv.show_frame_in_notebook(frame, (16, 16))"]},{"cell_type":"markdown","metadata":{"id":"ROVuD0XHf1iw"},"source":["Our goal is primarily to detect people. Therefore, let's filter out detections related to other classes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iPLH7fufBoUb"},"outputs":[],"source":["import supervision as sv\n","\n","# extract video frame\n","generator = sv.get_video_frames_generator(MALL_VIDEO_PATH)\n","iterator = iter(generator)\n","frame = next(iterator)\n","\n","# detect\n","results = model(frame, imgsz=1280)[0]\n","detections = sv.Detections.from_yolov8(results)\n","detections = detections[detections.class_id == 0]\n","\n","# annotate\n","box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n","labels = [f\"{model.names[class_id]} {confidence:0.2f}\" for _, confidence, class_id, _ in detections]\n","frame = box_annotator.annotate(scene=frame, detections=detections, labels=labels)\n","\n","%matplotlib inline\n","sv.show_frame_in_notebook(frame, (16, 16))"]},{"cell_type":"markdown","metadata":{"id":"ozDnE-Iwh92Y"},"source":[" Now we can add a polygon zone to the scene and visualize it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VEnDHeW1Cvpw"},"outputs":[],"source":["sv.VideoInfo.from_video_path(MALL_VIDEO_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jxghPeiviW3v"},"outputs":[],"source":["import numpy as np\n","import supervision as sv\n","\n","# initiate polygon zone\n","polygon = np.array([\n","    [1900, 1250],\n","    [2350, 1250],\n","    [3500, 2160],\n","    [1250, 2160]\n","])\n","video_info = sv.VideoInfo.from_video_path(MALL_VIDEO_PATH)\n","zone = sv.PolygonZone(polygon=polygon, frame_resolution_wh=video_info.resolution_wh)\n","\n","# initiate annotators\n","box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n","zone_annotator = sv.PolygonZoneAnnotator(zone=zone, color=sv.Color.white(), thickness=6, text_thickness=6, text_scale=4)\n","\n","# extract video frame\n","generator = sv.get_video_frames_generator(MALL_VIDEO_PATH)\n","iterator = iter(generator)\n","frame = next(iterator)\n","\n","# detect\n","results = model(frame, imgsz=1280)[0]\n","detections = sv.Detections.from_yolov8(results)\n","detections = detections[detections.class_id == 0]\n","zone.trigger(detections=detections)\n","\n","# annotate\n","box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n","labels = [f\"{model.names[class_id]} {confidence:0.2f}\" for _, confidence, class_id, _ in detections]\n","frame = box_annotator.annotate(scene=frame, detections=detections, labels=labels)\n","frame = zone_annotator.annotate(scene=frame)\n","\n","%matplotlib inline\n","sv.show_frame_in_notebook(frame, (16, 16))"]},{"cell_type":"markdown","metadata":{"id":"tvUxdHTYy0B3"},"source":["The number 1 visible in the middle of the zone we painted. Now let's try changing its location and see what happens."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4joTbIRWzRvp"},"outputs":[],"source":["import numpy as np\n","import supervision as sv\n","\n","# initiate polygon zone\n","polygon = np.array([\n","    [1725, 1550],\n","    [2725, 1550],\n","    [3500, 2160],\n","    [1250, 2160]\n","])\n","video_info = sv.VideoInfo.from_video_path(MALL_VIDEO_PATH)\n","zone = sv.PolygonZone(polygon=polygon, frame_resolution_wh=video_info.resolution_wh)\n","\n","# initiate annotators\n","box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n","zone_annotator = sv.PolygonZoneAnnotator(zone=zone, color=sv.Color.white(), thickness=6, text_thickness=6, text_scale=4)\n","\n","# extract video frame\n","generator = sv.get_video_frames_generator(MALL_VIDEO_PATH)\n","iterator = iter(generator)\n","frame = next(iterator)\n","\n","# detect\n","results = model(frame, imgsz=1280)[0]\n","detections = sv.Detections.from_yolov8(results)\n","detections = detections[detections.class_id == 0]\n","zone.trigger(detections=detections)\n","\n","# annotate\n","box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n","labels = [f\"{model.names[class_id]} {confidence:0.2f}\" for _, confidence, class_id, _ in detections]\n","frame = box_annotator.annotate(scene=frame, detections=detections, labels=labels)\n","frame = zone_annotator.annotate(scene=frame)\n","\n","%matplotlib inline\n","sv.show_frame_in_notebook(frame, (16, 16))"]},{"cell_type":"markdown","metadata":{"id":"NSIVKBdnz2nP"},"source":["now no person is in the zone, and consequently the counter still indicates zero.\n","\n","Now let's try to process the whole video."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OuTCMzHl1_Kq"},"outputs":[],"source":["import numpy as np\n","import supervision as sv\n","\n","# initiate polygon zone\n","polygon = np.array([\n","    [1725, 1550],\n","    [2725, 1550],\n","    [3500, 2160],\n","    [1250, 2160]\n","])\n","video_info = sv.VideoInfo.from_video_path(MALL_VIDEO_PATH)\n","zone = sv.PolygonZone(polygon=polygon, frame_resolution_wh=video_info.resolution_wh)\n","\n","# initiate annotators\n","box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n","zone_annotator = sv.PolygonZoneAnnotator(zone=zone, color=sv.Color.white(), thickness=6, text_thickness=6, text_scale=4)\n","\n","def process_frame(frame: np.ndarray, _) -> np.ndarray:\n","    # detect\n","    results = model(frame, imgsz=1280)[0]\n","    detections = sv.Detections.from_yolov8(results)\n","    detections = detections[detections.class_id == 0]\n","    zone.trigger(detections=detections)\n","\n","    # annotate\n","    box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n","    labels = [f\"{model.names[class_id]} {confidence:0.2f}\" for _, confidence, class_id, _ in detections]\n","    frame = box_annotator.annotate(scene=frame, detections=detections, labels=labels)\n","    frame = zone_annotator.annotate(scene=frame)\n","\n","    return frame\n","\n","sv.process_video(source_path=MALL_VIDEO_PATH, target_path=f\"{HOME}/mall-result.mp4\", callback=process_frame)\n","\n","from IPython import display\n","display.clear_output()"]},{"cell_type":"markdown","metadata":{"id":"7XTzvZYc-yZR"},"source":["## Simple Detectron2 Subway"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6DcC4bhI_ROi"},"outputs":[],"source":["from detectron2 import model_zoo\n","from detectron2.config import get_cfg\n","from detectron2.engine import DefaultPredictor\n","\n","\n","cfg = get_cfg()\n","cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n","cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n","cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n","predictor = DefaultPredictor(cfg)"]},{"cell_type":"markdown","metadata":{"id":"yjkO61Pr-527"},"source":["Let's start vanilla YOLOv8 inference pipeline."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SFvBC9xl-8Bv"},"outputs":[],"source":["import supervision as sv\n","\n","# extract video frame\n","generator = sv.get_video_frames_generator(SUBWAY_VIDEO_PATH)\n","iterator = iter(generator)\n","frame = next(iterator)\n","\n","# detect\n","outputs = predictor(frame)\n","detections = sv.Detections(\n","    xyxy=outputs[\"instances\"].pred_boxes.tensor.cpu().numpy(),\n","    confidence=outputs[\"instances\"].scores.cpu().numpy(),\n","    class_id=outputs[\"instances\"].pred_classes.cpu().numpy().astype(int)\n",")\n","\n","# annotate\n","box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n","frame = box_annotator.annotate(scene=frame, detections=detections)\n","\n","%matplotlib inline\n","sv.show_frame_in_notebook(frame, (16, 16))"]},{"cell_type":"markdown","metadata":{"id":"pFNAGCLW_hE1"},"source":["Once again, we have a lot of excess detections that we are not interested in. Let us remove all those not belonging to the person class and hide labels for better visibility."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qk8i4jlN-1vB"},"outputs":[],"source":["import supervision as sv\n","\n","# extract video frame\n","generator = sv.get_video_frames_generator(SUBWAY_VIDEO_PATH)\n","iterator = iter(generator)\n","frame = next(iterator)\n","\n","# detect\n","outputs = predictor(frame)\n","detections = sv.Detections(\n","    xyxy=outputs[\"instances\"].pred_boxes.tensor.cpu().numpy(),\n","    confidence=outputs[\"instances\"].scores.cpu().numpy(),\n","    class_id=outputs[\"instances\"].pred_classes.cpu().numpy().astype(int)\n",")\n","detections = detections[detections.class_id == 0]\n","\n","# annotate\n","box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n","frame = box_annotator.annotate(scene=frame, detections=detections, skip_label=True)\n","\n","%matplotlib inline\n","sv.show_frame_in_notebook(frame, (16, 16))"]},{"cell_type":"markdown","metadata":{"id":"TYDkIpjBB3SF"},"source":["Now we can add a polygon zone to the scene and visualize it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xlocfoLQCskP"},"outputs":[],"source":["sv.VideoInfo.from_video_path(SUBWAY_VIDEO_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vg9g0yiMCGzJ"},"outputs":[],"source":["import numpy as np\n","import supervision as sv\n","\n","# initiate polygon zone\n","polygon = np.array([\n","    [200, 3840],\n","    [1300, 600],\n","    [1325, 600],\n","    [550, 3840]\n","])\n","video_info = sv.VideoInfo.from_video_path(SUBWAY_VIDEO_PATH)\n","zone = sv.PolygonZone(polygon=polygon, frame_resolution_wh=video_info.resolution_wh)\n","\n","# initiate annotators\n","box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n","zone_annotator = sv.PolygonZoneAnnotator(zone=zone, color=sv.Color.white(), thickness=6, text_thickness=6, text_scale=4)\n","\n","# extract video frame\n","generator = sv.get_video_frames_generator(SUBWAY_VIDEO_PATH)\n","iterator = iter(generator)\n","frame = next(iterator)\n","\n","# detect\n","outputs = predictor(frame)\n","detections = sv.Detections(\n","    xyxy=outputs[\"instances\"].pred_boxes.tensor.cpu().numpy(),\n","    confidence=outputs[\"instances\"].scores.cpu().numpy(),\n","    class_id=outputs[\"instances\"].pred_classes.cpu().numpy().astype(int)\n",")\n","detections = detections[detections.class_id == 0]\n","zone.trigger(detections=detections)\n","\n","# annotate\n","box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n","frame = box_annotator.annotate(scene=frame, detections=detections, skip_label=True)\n","frame = zone_annotator.annotate(scene=frame)\n","\n","%matplotlib inline\n","sv.show_frame_in_notebook(frame, (16, 16))"]},{"cell_type":"markdown","metadata":{"id":"1tZfhDUnH6UF"},"source":["Now let's process the whole video."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ISgPn1-MH5zM"},"outputs":[],"source":["import numpy as np\n","import supervision as sv\n","\n","# initiate polygon zone\n","polygon = np.array([\n","    [200, 3840],\n","    [1300, 600],\n","    [1325, 600],\n","    [550, 3840]\n","])\n","video_info = sv.VideoInfo.from_video_path(SUBWAY_VIDEO_PATH)\n","zone = sv.PolygonZone(polygon=polygon, frame_resolution_wh=video_info.resolution_wh)\n","\n","# initiate annotators\n","box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n","zone_annotator = sv.PolygonZoneAnnotator(zone=zone, color=sv.Color.white(), thickness=6, text_thickness=6, text_scale=4)\n","\n","def process_frame(frame: np.ndarray, i: int) -> np.ndarray:\n","    print('frame', i)\n","    # detect\n","    outputs = predictor(frame)\n","    detections = sv.Detections(\n","        xyxy=outputs[\"instances\"].pred_boxes.tensor.cpu().numpy(),\n","        confidence=outputs[\"instances\"].scores.cpu().numpy(),\n","        class_id=outputs[\"instances\"].pred_classes.cpu().numpy().astype(int)\n","    )\n","    detections = detections[detections.class_id == 0]\n","    zone.trigger(detections=detections)\n","\n","    # annotate\n","    box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n","    frame = box_annotator.annotate(scene=frame, detections=detections, skip_label=True)\n","    frame = zone_annotator.annotate(scene=frame)\n","\n","    return frame\n","\n","sv.process_video(source_path=SUBWAY_VIDEO_PATH, target_path=f\"{HOME}/subway-result.mp4\", callback=process_frame)\n","\n","from IPython import display\n","display.clear_output()"]},{"cell_type":"markdown","metadata":{"id":"X-0rnX_3P0Nq"},"source":["## Advanced YOLOv5 Market Square"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0EL4o_eOQmej"},"outputs":[],"source":["import torch\n","\n","model = torch.hub.load('ultralytics/yolov5', 'yolov5x6')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9u4Bp6QAQsIN"},"outputs":[],"source":["import supervision as sv\n","\n","# extract video frame\n","generator = sv.get_video_frames_generator(MARKET_SQUARE_VIDEO_PATH)\n","iterator = iter(generator)\n","frame = next(iterator)\n","\n","# detect\n","results = model(frame, size=1280)\n","detections = sv.Detections.from_yolov5(results)\n","\n","# annotate\n","box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n","frame = box_annotator.annotate(scene=frame, detections=detections)\n","\n","%matplotlib inline\n","sv.show_frame_in_notebook(frame, (16, 16))"]},{"cell_type":"markdown","metadata":{"id":"kfmxpZgbRo91"},"source":["Once again, we have a lot of excess detections that we are not interested in. Let us remove all those not belonging to the person class. At the same time (to show off), we can reject all detections not exceeding `0.5` confidence."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S4SYSJEES1SX"},"outputs":[],"source":["import supervision as sv\n","\n","# extract video frame\n","generator = sv.get_video_frames_generator(MARKET_SQUARE_VIDEO_PATH)\n","iterator = iter(generator)\n","frame = next(iterator)\n","\n","# detect\n","results = model(frame, size=1280)\n","detections = sv.Detections.from_yolov5(results)\n","detections = detections[(detections.class_id == 0) & (detections.confidence > 0.5)]\n","\n","# annotate\n","box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n","frame = box_annotator.annotate(scene=frame, detections=detections)\n","\n","%matplotlib inline\n","sv.show_frame_in_notebook(frame, (16, 16))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lhjsYhHWUTBe"},"outputs":[],"source":["len(detections)"]},{"cell_type":"markdown","metadata":{"id":"gqBjWCu6UXYI"},"source":["**NOTE:** We can use `Detection` API to easly calculate how many people we see on the frame. But what if we would like to divide the Market Square into smaller zones and know how many people we see each zone."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ul_e75nuaVZj"},"outputs":[],"source":["import numpy as np\n","import supervision as sv\n","\n","# initiate polygon zone\n","polygon = np.array([\n","    [0, 0],\n","    [1080 - 5, 0],\n","    [1080 - 5, 1300 - 5],\n","    [0, 1300 - 5]\n","])\n","video_info = sv.VideoInfo.from_video_path(MARKET_SQUARE_VIDEO_PATH)\n","zone = sv.PolygonZone(polygon=polygon, frame_resolution_wh=video_info.resolution_wh)\n","\n","# extract video frame\n","generator = sv.get_video_frames_generator(MARKET_SQUARE_VIDEO_PATH)\n","iterator = iter(generator)\n","frame = next(iterator)\n","\n","# detect\n","results = model(frame, size=1280)\n","detections = sv.Detections.from_yolov5(results)\n","mask = zone.trigger(detections=detections)\n","detections = detections[(detections.class_id == 0) & (detections.confidence > 0.5) & mask]\n","\n","# annotate\n","box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n","frame = box_annotator.annotate(scene=frame, detections=detections)\n","frame = sv.draw_polygon(scene=frame, polygon=polygon, color=sv.Color.red(), thickness=6)\n","\n","%matplotlib inline\n","sv.show_frame_in_notebook(frame, (16, 16))"]},{"cell_type":"markdown","metadata":{"id":"Iv0bFpvMbMK6"},"source":["Importantly, zones can have a much more complex geometry."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xz32GaDdcx5c"},"outputs":[],"source":["sv.VideoInfo.from_video_path(MARKET_SQUARE_VIDEO_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k8FlU4jYet9E"},"outputs":[],"source":["import numpy as np\n","import supervision as sv\n","\n","# initiate polygon zone\n","polygon = np.array([\n","    [540,  985],\n","    [1620, 985],\n","    [2160, 1920],\n","    [1620, 2855],\n","    [540,  2855],\n","    [0,    1920]\n","])\n","video_info = sv.VideoInfo.from_video_path(MARKET_SQUARE_VIDEO_PATH)\n","zone = sv.PolygonZone(polygon=polygon, frame_resolution_wh=video_info.resolution_wh)\n","\n","# extract video frame\n","generator = sv.get_video_frames_generator(MARKET_SQUARE_VIDEO_PATH)\n","iterator = iter(generator)\n","frame = next(iterator)\n","\n","# detect\n","results = model(frame, size=1280)\n","detections = sv.Detections.from_yolov5(results)\n","mask = zone.trigger(detections=detections)\n","detections = detections[(detections.class_id == 0) & (detections.confidence > 0.5) & mask]\n","\n","# annotate\n","box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n","frame = box_annotator.annotate(scene=frame, detections=detections)\n","frame = sv.draw_polygon(scene=frame, polygon=polygon, color=sv.Color.red(), thickness=6)\n","\n","%matplotlib inline\n","sv.show_frame_in_notebook(frame, (16, 16))"]},{"cell_type":"markdown","metadata":{"id":"I_kq3Uub1hfG"},"source":["**NOTE:** We can also have more complex behaviours."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gDvLOtzMf-o1"},"outputs":[],"source":["colors = sv.ColorPalette.default()\n","polygons = [\n","    np.array([\n","        [0, 0],\n","        [1080 - 5, 0],\n","        [1080 - 5, 1300 - 5],\n","        [0, 1300 - 5]\n","    ], np.int32),\n","    np.array([\n","        [1080 + 5, 0],\n","        [2160, 0],\n","        [2160, 1300 - 5],\n","        [1080 + 5, 1300 - 5]\n","    ], np.int32),\n","    np.array([\n","        [0, 1300 + 5],\n","        [1080 - 5, 1300 + 5],\n","        [1080 - 5, 3840],\n","        [0, 3840]\n","    ], np.int32),\n","    np.array([\n","        [1080 + 5, 1300 + 5],\n","        [2160, 1300 + 5],\n","        [2160, 3840],\n","        [1080 + 5, 3840]\n","    ], np.int32)\n","]\n","video_info = sv.VideoInfo.from_video_path(MARKET_SQUARE_VIDEO_PATH)\n","\n","zones = [\n","    sv.PolygonZone(\n","        polygon=polygon,\n","        frame_resolution_wh=video_info.resolution_wh\n","    )\n","    for polygon\n","    in polygons\n","]\n","zone_annotators = [\n","    sv.PolygonZoneAnnotator(\n","        zone=zone,\n","        color=colors.by_idx(index),\n","        thickness=4,\n","        text_thickness=8,\n","        text_scale=4\n","    )\n","    for index, zone\n","    in enumerate(zones)\n","]\n","box_annotators = [\n","    sv.BoxAnnotator(\n","        color=colors.by_idx(index),\n","        thickness=4,\n","        text_thickness=4,\n","        text_scale=2\n","        )\n","    for index\n","    in range(len(polygons))\n","]\n","\n","# extract video frame\n","generator = sv.get_video_frames_generator(MARKET_SQUARE_VIDEO_PATH)\n","iterator = iter(generator)\n","frame = next(iterator)\n","\n","# detect\n","results = model(frame, size=1280)\n","detections = sv.Detections.from_yolov5(results)\n","detections = detections[(detections.class_id == 0) & (detections.confidence > 0.5)]\n","\n","for zone, zone_annotator, box_annotator in zip(zones, zone_annotators, box_annotators):\n","    mask = zone.trigger(detections=detections)\n","    detections_filtered = detections[mask]\n","    frame = box_annotator.annotate(scene=frame, detections=detections_filtered)\n","    frame = zone_annotator.annotate(scene=frame)\n","\n","%matplotlib inline\n","sv.show_frame_in_notebook(frame, (16, 16))"]},{"cell_type":"markdown","metadata":{"id":"03f-lTPJ1uHf"},"source":["**NOTE:** Or we can go compleatly crazy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z8qahsKWtUI2"},"outputs":[],"source":["colors = sv.ColorPalette.default()\n","polygons = [\n","    np.array([\n","        [540,  985 ],\n","        [1620, 985 ],\n","        [2160, 1920],\n","        [1620, 2855],\n","        [540,  2855],\n","        [0,    1920]\n","    ], np.int32),\n","    np.array([\n","        [0,    1920],\n","        [540,  985 ],\n","        [0,    0   ]\n","    ], np.int32),\n","    np.array([\n","        [1620, 985 ],\n","        [2160, 1920],\n","        [2160,    0]\n","    ], np.int32),\n","    np.array([\n","        [540,  985 ],\n","        [0,    0   ],\n","        [2160, 0   ],\n","        [1620, 985 ]\n","    ], np.int32),\n","    np.array([\n","        [0,    1920],\n","        [0,    3840],\n","        [540,  2855]\n","    ], np.int32),\n","    np.array([\n","        [2160, 1920],\n","        [1620, 2855],\n","        [2160, 3840]\n","    ], np.int32),\n","    np.array([\n","        [1620, 2855],\n","        [540,  2855],\n","        [0,    3840],\n","        [2160, 3840]\n","    ], np.int32)\n","]\n","video_info = sv.VideoInfo.from_video_path(MARKET_SQUARE_VIDEO_PATH)\n","\n","zones = [\n","    sv.PolygonZone(\n","        polygon=polygon,\n","        frame_resolution_wh=video_info.resolution_wh\n","    )\n","    for polygon\n","    in polygons\n","]\n","zone_annotators = [\n","    sv.PolygonZoneAnnotator(\n","        zone=zone,\n","        color=colors.by_idx(index),\n","        thickness=6,\n","        text_thickness=8,\n","        text_scale=4\n","    )\n","    for index, zone\n","    in enumerate(zones)\n","]\n","box_annotators = [\n","    sv.BoxAnnotator(\n","        color=colors.by_idx(index),\n","        thickness=4,\n","        text_thickness=4,\n","        text_scale=2\n","        )\n","    for index\n","    in range(len(polygons))\n","]\n","\n","# extract video frame\n","generator = sv.get_video_frames_generator(MARKET_SQUARE_VIDEO_PATH)\n","iterator = iter(generator)\n","frame = next(iterator)\n","\n","# detect\n","results = model(frame, size=1280)\n","detections = sv.Detections.from_yolov5(results)\n","detections = detections[(detections.class_id == 0) & (detections.confidence > 0.5)]\n","\n","for zone, zone_annotator, box_annotator in zip(zones, zone_annotators, box_annotators):\n","    mask = zone.trigger(detections=detections)\n","    detections_filtered = detections[mask]\n","    frame = box_annotator.annotate(scene=frame, detections=detections_filtered, skip_label=True)\n","    frame = zone_annotator.annotate(scene=frame)\n","\n","%matplotlib inline\n","sv.show_frame_in_notebook(frame, (16, 16))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WUat33Tgs__w"},"outputs":[],"source":["colors = sv.ColorPalette.default()\n","polygons = [\n","    np.array([\n","        [540,  985 ],\n","        [1620, 985 ],\n","        [2160, 1920],\n","        [1620, 2855],\n","        [540,  2855],\n","        [0,    1920]\n","    ], np.int32),\n","    np.array([\n","        [0,    1920],\n","        [540,  985 ],\n","        [0,    0   ]\n","    ], np.int32),\n","    np.array([\n","        [1620, 985 ],\n","        [2160, 1920],\n","        [2160,    0]\n","    ], np.int32),\n","    np.array([\n","        [540,  985 ],\n","        [0,    0   ],\n","        [2160, 0   ],\n","        [1620, 985 ]\n","    ], np.int32),\n","    np.array([\n","        [0,    1920],\n","        [0,    3840],\n","        [540,  2855]\n","    ], np.int32),\n","    np.array([\n","        [2160, 1920],\n","        [1620, 2855],\n","        [2160, 3840]\n","    ], np.int32),\n","    np.array([\n","        [1620, 2855],\n","        [540,  2855],\n","        [0,    3840],\n","        [2160, 3840]\n","    ], np.int32)\n","]\n","video_info = sv.VideoInfo.from_video_path(MARKET_SQUARE_VIDEO_PATH)\n","\n","zones = [\n","    sv.PolygonZone(\n","        polygon=polygon,\n","        frame_resolution_wh=video_info.resolution_wh\n","    )\n","    for polygon\n","    in polygons\n","]\n","zone_annotators = [\n","    sv.PolygonZoneAnnotator(\n","        zone=zone,\n","        color=colors.by_idx(index),\n","        thickness=6,\n","        text_thickness=8,\n","        text_scale=4\n","    )\n","    for index, zone\n","    in enumerate(zones)\n","]\n","box_annotators = [\n","    sv.BoxAnnotator(\n","        color=colors.by_idx(index),\n","        thickness=4,\n","        text_thickness=4,\n","        text_scale=2\n","        )\n","    for index\n","    in range(len(polygons))\n","]\n","\n","def process_frame(frame: np.ndarray, i) -> np.ndarray:\n","    print(i)\n","    # detect\n","    results = model(frame, size=1280)\n","    detections = sv.Detections.from_yolov5(results)\n","    detections = detections[(detections.class_id == 0) & (detections.confidence > 0.5)]\n","\n","    for zone, zone_annotator, box_annotator in zip(zones, zone_annotators, box_annotators):\n","        mask = zone.trigger(detections=detections)\n","        detections_filtered = detections[mask]\n","        frame = box_annotator.annotate(scene=frame, detections=detections_filtered, skip_label=True)\n","        frame = zone_annotator.annotate(scene=frame)\n","\n","    return frame\n","\n","sv.process_video(source_path=MARKET_SQUARE_VIDEO_PATH, target_path=f\"{HOME}/market-square-result.mp4\", callback=process_frame)\n","\n","from IPython import display\n","display.clear_output()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1NsKgMQRQGIZRjhpb2hd9HsBjfHHz81nD","timestamp":1733939394320},{"file_id":"1eZ1IF9SfmBIJxBCPKX_ncv7pgnnuXjEO","timestamp":1681064619797},{"file_id":"https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-detect-and-count-objects-in-polygon-zone.ipynb","timestamp":1677049101901}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}